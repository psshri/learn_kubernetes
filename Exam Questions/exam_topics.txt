Pods and labels
Deployments
Services
PV and PVCs
RBAC
Cluster Upgrade
ETCD Backup and Restore
Manual Scheduling
Metrics
Network Policies
Volumes
Debugging
Ingress
Namespaces
Daemonsets
Configmaps
Secrets
JSONPath

Common Questions
-------------------
-ETCD backup and restore
-Upgrade cluster to v1.2x.x
-RBAC: ClusterRole, ClusterRoleBinding, Role, RoleBinding, Service Accounts
-Find nodes that are ready and taints not set as NoSchedule
-Find pod that is consuming maximum CPU from all the pods in a given namespace
-Add a sidecar container that prints the logs of main container. You need to 
create emptyDir and mount to both the containers. Command to print logs will be
given
-Create a PVC of 10Mi and mount it to a container. Later patch PVC to 70Mi.
-Create a pod with 2 containers. Image names and other info will be provided
-Scale a deployment to 3 replicas
-Create a deployment with X image and Y tag. Change the Y tag to Z and record
the changes
-Create a pod and manually schedule it with NodeSelector(disk:ssd)
-Schedule a pod on master node without modifying labels on the node
-Drain all workloads from a node
-Create a network policy that allows all pods in Y namespace to talk to all 
pods in X namespace on port 5432
-A kubernetes worker node is in state NotReady. Investigate why this is the case,
and perform any appropriate steps to bring the node to a ready state
-Setting CPU and memory limits to containers of a pod
-Security contexts to add user id and group id to pod, add additional capabilities
like SYS_TIME and NET_ADMIN
-Sort all pods by their order of creation
-Write all namespaced resources to a file
-Filter pods by certain labels and write the results to a file
-Delete kube-proxy container from Y node. Use crictl ps and crictl rm 
-Create a pod with given labels
-Add a secondary container that runs certain commands like sleep 2000
-Write a command to list all clusters from kubeconfig file into file
-Create a pod with some environment variables
-Mount an existing secret as a volume

-----------------------------------------------------------------

ETCD DATABASE BACKUP AND RESTORE

types of questions
1) backup 
2) restore

etcd starts a service that listens on port 2379 by default

if you install k8s using kubeadm, then etcd is provisioned as a static pod 
inside kube-system namespace 

kubectl describe pods etcd-controlplane -n kube-system

backing up an etcd cluster can be accomplished in two ways: etcd built-in 
snapshot or volume snapshot 

go to 'operating etcd clusters for kubernetes' and go to 
backing up an etcd cluster->volume snapshot->snapshot using etcdctl options

ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 \
  --cacert=<trusted-ca-file> --cert=<cert-file> --key=<key-file> \
  snapshot save <backup-file-location>

fill in values from describe command and in backupfilelocation fill
/opt/snapshot-pre-boot.db

follow the below steps to restore the database

ETCDCTL_API=3 etcdctl snapshot restore --data-dir <data-dir-location> snapshotdb

ETCDCTL_API=3 etcdctl snapshot restore --data-dir=/var/lib/etcd-backup /opt/snapshot-pre-boot.db

now direct the manifest file of etcd to use this backup

go to /etc/kubernetes/manifests

vim etcd.yaml

update the path under hostPath 


if certificate file location is given in question then use it otherwise use
the one mentioned in describe command

==============================================================================

PERSISTENT VOLUME AND PERSISTENT VOLUME CLAIM

the pvc and the pod that uses that pvc has to be in the same namespace. pv will
always be a part of the default namespace

pv.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: mypvlog
spec:
  capacity:
    storage: 100Mi
  accessModes: 
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  hostPath: 
    path: /pv/log

pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pv-claim-log
spec:
  accessModes:
    - ReadWriteMany
  resources: 
    requests:
      storage: 50Mi

pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-nginx-pod
spec: 
  containers:
  - name: mypod
    image: nginx
    volumeMounts: 
    - mountPath: /log
      name: mypd
  volumes:
  - name: mypd
    persistentVolumeClaim: 
      claimName: pv-claim-log

pv-pvc is 1-1 mapping only

pv is not namespaced
pvc is namespaced
pods are namespaced

=========================================================================

TAINTS & TOLERATIONS

kubectl taint nodes node01 key1=value1:NoSchedule

above command will place a taint on node node01. the taint has key key1, value
value1 and taint effect NoSchedule. this means that no pod will be able to 
schedule onto node01 unless it has a matching toleration. if a pod has a matching
toleration it can be deployed on node01 and other nodes as well which has no
restrictions 

to remove the taint added by the command above, you can run:

kubectl taint node node01 key1=value1:NoSchedule-


NoSchedule: the pod will not be scheduled on the node
PreferNoSchedule: the system will try to avoid placing a pod on the node but
that is not guaranteed
NoExecute: new pod will not be scheduled on the node and existing pods on the node,
if any will be evicted, if they do not tolerate the taint

vim pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    env: test
spec:
  containers:
  - name: nginx
    image: nginx
  tolerations:
  - key: "key1"
    operator: "Equal"
    value: "value1"
    effect: "NoSchedule"

kubectl describe node controlplane | grep -i taint

*****************************************************************************

ROLE, ROLEBINDING, CLUSTERROLE, CLUSTERROLEBINDING

below is an example of a role in the default namespace that can be used to 
grant read access to pods

role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: default
  name: pod-reader
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get","watch","list"]

below is an example of clusterrole that can be used to grant read access
to secrets in any particular namespace or across all namespaces (depending on
how it is bound)

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: secret-reader
rules:
- apiGroups: [""]
  resources: ["secrets"]
  verbs: ["get","watch","list"]

role bindings grants the permissions defined in role to a user or group

rolebindings example
here is an example of a rolebinding that grants the pods-reader role to the 
user jane within the default namespace. this allows jane to read pods in the 
default namespace

apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding 
metadata:
  name: read-pods
  namespace: default
subjects:
- kind: User
  name: jane
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io


a rolebinding can also reference a clusterrole to grant the permissions 
defined in that clusterrole to resources inside the rolebinding's namespace.
this kind of reference lets you define a set of common roles across your cluster
then reuse them within multiple namespaces

for example, even though the following rolebinding refers to a clusterrole, dave
will only be able to read secrets in the developement namespace, because the 
rolebinding's namespace is development

apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: read-secrets
  namespace: development
subjects:
- kind: User
  name: dave
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: secret-reader
  apiGroup: rbac.authorization.k8s.io


the following clusterrolebinding allows any user in the group manager to read
secrets in any namespace

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: read-secrets-global
subjects:
- kind: Group
  name: manager
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: secret-reader
  apiGroup: rbac.authorization.k8s.io

after you create a binding, you cannot change the role or clusterrole it refers 
to

***************************************************************************

CERTIFICATE SIGNING REQUEST, USER ADDITION

follow the below steps in order to get a normal user to be able to 
authenticate and invoke an API. first, this user must have a certificate 
issued by the kubernetes cluster, and then present that certificate to the 
kubernetes API

create a private key

openssl genrsa -out myuser.key 2048
openssl req -new -key myuser.key -out myuser.csr

create a csr
vim csr.yaml
apiVersion: certificates.k8s.io/v1
kind: CertificateSigningRequest
metadata:
  name: myuser
spec:
  request: 
  signerName: kubernetes.io/kube-apiserver-client
  usages:
  - client auth

request is the base64 encoded value of the csr file content. get the content
using below command
cat myuser.csr | base64 | tr -d "\n"

kubectl apply -f csr.yaml

use kubectl to create a csr and approve it
kubectl get csr
kubectl certificate approve myuser

the certificate value is in base64 encoded format under status.certificate. 
export the issued certificate from the certificatesigningrequest

kubectl get csr myuser -o jsonpath='{.status.certificate}' | base64 -d > myuser.crt

with certificate created it is time to define role and rolebinding for this user
to access kubernetes cluster resources

vim role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: default
  name: developer
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["create", "get", "list", "update", "delete"]

vim rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: developer-binding-myuser
  namespace: default
subjects:
- kind: User
  name: myuser
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: developer
  apiGroup: rbac.authorization.k8s.io

kubectl apply -f role.yaml
kubectl apply -f rolebinding.yaml

add this user to kubeconfig file
kubectl config set-credentials myuser --client-key=myuser.key --client-certificate=myuser.crt --embed-certs=true
kubectl config set-context myuser --cluster=kubernetes --user=myuser
kubectl config use-context myuser

************************************************************************

REPLICASET  

the ReplicaSet ensures that the specified number of pods are running at all times

another reason we need replicaset is to create multiple pods to share the load
across them. balances the load across multiple pods on different nodes,
when demand increases

the replicaset spans across the multiple nodes in the cluster

automatic rollback to a previous version is not possible with replicaset,
that's why deployments are used 

vim replicaset.yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: replicaset-example
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      name: nginx-pod
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14

kubectl apply -f replicaset.yaml
kubectl get rs

if you now update the image version of rs file, then it will be updated in the 
rs but not in pod. so this is one disadvantage of rs, if you want to update 
the image version, then you first need to delete the replicaset then apply it
again

if you create a pod and label it as key1:value1 and then if you create a rs
and set replicas as 4 and if you set the label in rs as key1:value1, then the
number of pods created by rs will be just 3 and not 4, because rs's job is
to make sure that in total 4 pods should be running with matching labels and 
since 1 is already running, therefore it will create just 3. so make sure that
your bare pods or any other pods do not have labels which match selector of one
of your replicas 

************************************************************************

DEPLOYMENTS

deployments comes with the feature of rollback

with deployments you can perform update in a rollout manner to ensure that 
your application is not down

deployments works on top of replicasets and replicasets works on top of pods

deployment ensures that only a certain number of pods are down while they are
being updated. by default, deployment ensures that at least 25% of the desired
number of pods are up

deployment keeps the history of revision which has been made

vim deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      name: nginx-pod
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx-container
        image: nginx



***********************************************************************

DEBUGGIN NODES

kubectl debug node/node01 -it --image=ubuntu
use the above command if you cannot access the node using ssh, the above
command opens an interactive shell on the node

************************************************************************

TROUBLESHOOTING APPLICATIONS

DEBUG PODS

the first step is to take a loot at it, check the current state of the pod and recent events
kubectl describe pod {pod_name}

look at the state of the containers in the pod. are they all running? have there been recent restarts?

if a pod is stuck in pending, it means that it cannot be scheduled onto a node. generally this is because
there are insufficient resources of one type or another that prevent scheduling. look at the output of 
kubectl describe command above, there should be messages from the scheduler about why it cannot 
schedule your pod.

-reason: you don't have enough resources: CPU/memory exhausted in the cluster, in this case
you need to delete pods, adjust resource requests or add new nodes to your cluster
-reason: you are using hostPort: when you bind a pod to a hostPort there are a limited number of places
that pod can be scheduled. in most cases, hostPort is unnecessary, try using a service object to expose 
your pod. if you do require hostPort then you can only schedule as many pods as there are nodes in 
your kubernetes cluster. 

if a pod is stuck in the waiting state, then it has been scheduled to a worker node, but it can't run
on that machine. info from kubectl describe should be useful. the most common cause of waiting pods 
is a failure to pull the image. there are three things to check
1) make sure you have the name of the image correct
2) have you pushed the image to the registry
3) try to manually pull the image to see if the image can be pulled. docker pull <imagename>

if a pod is crashing or otherwise unhealthy
refer to debug running pods

if a pod is running but not doing its intended purpose: there may be an error in the pod yaml file and
that error was silently ignored when you created the pod. ususal mistakes are nesting issues and typos
the first thing to do is to delete your pod and create it again with --validate option 
kubectl apply -f mypod.yaml --validate
it highlights any spelling mistakes as well

the next thing to check is whether the pod on the apiserver matches the pod you meant to create . run 
kubectl get pod mypod -o yaml > mypod-on-apiserver.yaml and then manually compare the original 
pod description, if there are lines in the original yaml file that are not present in the apiserver file
then it indicates problem with pod spec

------------------------------------------------------------------------------------------

DEBUG REPLICATION CONTROLLERS

replication controllers are quite straightforward, if they can't create pods then debug your pods
you can also use kubectl describe rc {rcname}

-------------------------------------------------------------------------------------------

DEBUG SERVICES

there are several problems that can make service not run properly.
First, verify that there are endpoints for the service 
kubectl get endpoints {servicename}
make sure that the endpoints match up with the number of pods that you expect to be members of your
service 

if endpoints are missing, try listing pods using the labels that service uses 
kubectl get pods --selector=key1=value1,key2=value2

verify that the pod's containerPort matches up with service's targetPort

first step is to make sure that the service has been created

you also need to make sure that there are no network policy ingress rules which may affect incoming 
traffic to your pods 

if your service is running, has endpoints, and your pods are actually serving. at this point 
the whole service proxy mechanism is suspect. 

check whether kube-proxy is running?
ps auxw | grep kube-proxy

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

NETWORK POLICIES

vim networkpolicy.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: deny-network-policy
  namespace: default
spec:
  podSelector: 
    matchLabels:
      run: pod01
  policyTypes:
  - Ingress
  - Egress

demo
create two pods
vim pod1.yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod1
  labels:
    app: pod1
spec:
  containers:
  - name: pod1
    image: busybox
    command: ["sleep","4800"]

vim pod2.yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod2
  labels:
    app: pod2
spec:
  containers:
  - name: pod2
    image: busybox
    command: ["sleep","4800"]

kubectl apply -f pod1.yaml
kubectl apply -f pod2.yaml

lets access pod2 from inside of pod1

kubectl exec -it pod2 -- sh
ping <ip-pod1>

you will be able to reach pod1 from pod2, now create a network policy to
restrict ingress traffic to pod1

pod1 will not be able to accept any incoming traffic 
vim netpol.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: deny-network-policy
  namespace: default
spec:
  podSelector:
    matchLabels:
      app: pod1
  policyTypes:
  - Ingress 

kubectl exec -it pod2 -- sh
ping <ip-pod1>
ping will not work this time



another example of network policy

vim networkpolicy.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: test-network-policy
  namespace: default
spec:
  podSelector:
    matchLabels:
      role: db
  policyTypes:
    - Ingress
    - Egress
  ingress:
    - from:
        - ipBlock:
            cidr: 172.17.0.0/16
            except:
              - 172.17.1.0/24
        - namespaceSelector:
            matchLabels:
              project: myproject
        - podSelector:
            matchLabels:
              role: frontend
      ports:
        - protocol: TCP
          port: 6379
  egress:
    - to:
        - ipBlock:
            cidr: 10.0.0.0/24
      ports:
        - protocol: TCP
          port: 5978

podSelector selects the group of pods to which the policy applies. an empty
selector selects all pods in the namespace

policyTypes field indicates whether or not the given policy applies to ingress
traffic to selected pod or egress traffic from selected pod or both. by default
if nothing is specified, ingress is considered

ingress: traffic should match both the from and ports section 
egress: traffic matches both the to and ports section

so the above network policy isolates role=db pods in the default namespace 
for both ingress and egress traffic 

ingress rules allows connections to all pods in the default namespace 
with the label role=db on TCP port 6379 from:
  - any pod from the default namespace with label role=frontend
  - any pod in the namespace with label project=myproject
  - ipa in the ranges 172.17.0.0 - 172.17.0.255 and 172.17.2.0 - 172.17.255.255 
    (ie all of 172.17.0.0/16 except 172.17.1.0/24)

egress rules allows connections from any pod in the default namespace with the
label role=db to CIDR 10.0.0.0/24 on TCP port 5978

behaviour of to and from selectors
there are 4 kinds of selectors that can be specified in an ingress from 
section or egress to section

podSelector: this selects particular pods in the same namespace as the 
network policy which should be allowed as ingress sources or egress destinations

namespaceSelector: this selects particular namespaces for which all pods 
should be allowed as ingress sources or egress destinations

namespaceSelector and podSelector: selects particular pods within particular 
namespaces 

ipBlock: this selects particular ip cidr ranges to allow as ingress sources 
or egress destinations. these should be cluster-external ips, since pod ips 
are ephemeral and unpredictable 


example
ingress:
- from:
  - namespaceSelector:
      matchLabels:
        user: alice
    podSelector:
      matchLabels:
        role: client

above policy allows connections from pods with label role:client from the 
namespace having label user:alice

example:
ingress:
- from:
  - namespaceSelector:
      matchLabels:
        user: alice
  - podSelector:
      matchLabels:
        role: client

above network policy allows connections from pods in the local namespace with
the label role=client or from any pod in any namespace with label user=alice


default deny all ingress traffic
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-ingress
spec:
  podSelector: {}
  policyTypes: 
  - Ingress

allow all ingress traffic
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-all-ingress
spec:
  podSelector: {}
  ingress:
  - {}
  policyTypes:
  - Ingress


default deny all egress traffic
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-egress
spec:
  podSelector: {}
  policyTypes: 
  - Egress

allow all egress traffic
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-all-egress
spec:
  podSelector: {}
  policyTypes:
  - Egress
  egress:
  - {}

default deny all egress and ingress traffic
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: deny-egress-ingress
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress


targeting a range of ports
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: multi-egress-port
  namespace: default
spec:
  podSelector:
    matchLabels:
      role: db
  policyTypes: 
    - Egress 
  egress:
    - to: 
        - ipBlock:
            cidr: 10.0.0.0/24
      ports:
        - protocol: TCP
          port: 32000
          endPort: 32768

the above rule allows any pod with label role=db on the namespace default
to communicate with any IP within the range 10.0.0.0/24 over TCP, provided
that the targetPort is between the range 32000 and 32768

=========================================================================

CLUSTER UPGRADE

upgrade will be done using kubeadm 
upgrade will be carried out one node at a time, starting with master node
we need to drain the nodes before doing upgrade

master node upgrade command
kubeadm upgrade plan
kubectl drain controlplane --ignore-daemonsets
apt update
apt install kubeadm=1.19.0-00
kubeadm upgrade apply v1.19.0
apt install kubelet=1.19.0-00
systemctl restart kubelet
kubectl uncordon controlplane

worker node upgrade command
kubectl drain node01 --ignore-daemonsets
ssh node01
apt update
apt install kubeadm=1.19.0-00
kubeadm upgrade node
apt install kubelet=1.19.0-00
systemctl restart kubelet
kubectl uncordon node01

*************************************************************************

DAEMONSETS

a daemonset ensures that all (or some) nodes run a copy of a pod. as nodes are
added to the cluster, pods are added to them. As nodes are removed from the 
cluster, those pods are garbage collected. deleting a daemonset will clean up the 
pods it created. 

some use cases
- running a cluster storage daemon on every node
- running a log collection daemon on every node
- running a node monitoring daemon on every node

vim daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: antivirus
spec:
  selector: 
    matchLabels:
      tier: antivirus
  template:
    metadata:
      labels:
        tier: antivirus
    spec: 
      containers:
      - name: antivirus-container
        image: nginx

to delete daemonset pods, you need to delete daemonset file, use the following
command to delete the daemonset
kubectl delete -f daemonset.yaml
and do not use rm daemonset.yaml, isse yeh file bhi chale jayegi and 
you won't be able to delete the pods as well

running pods on select nodes
if you specify a .spec.template.spec.nodeSelector, then the daemonset controller
will create pods on nodes which match the node selector. likewise if you
specify a .spec.template.spec.affinity, then daemonset controller will create
pods on nodes which match that node affinity. if you do not specify either,
then the daemonset controller will create pods on all nodes. 

=============================================================================

NodeName and NodeSelector

Both NodeName and NodeSelector help to assign a pod to a particular node in a 
cluster 

apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - name: nginx
    image: nginx
  nodeName: controlplane

this pod will run on the controlplane

if you try to schedule a pod on a specific node by using nodeName parameter
and if you request resources more than what is available on that node then the
pod will not be scheduled, pod will go in pending state or out of memory in this
case. see the below example. even if there is some other node in the cluster that
have sufficient requirements, the pod will not be scheduled there

apiVersion: v1
kind: Pod
metadata:
  name: pod1
spec:
  containers:
  - name: nginx
    image: nginx
    resources:
      requests:
        memory: "2100Mi"
        cpu: "250m"

below is an example of nodeSelector
apiVersion: v1
kind: Pod
metadata: 
  name: pod1
  labels:
    app: pod1
spec:
  containers:
  - name: pod1
    image: nginx
  nodeSelector:
    disk: ssd

label a node with the below command
kubectl label node node01 disk=ssd

above pod will be scheduled on nodes having disk:ssd as a label

pods with nodeName in their yaml mainfest files are not sent to scheduler, they
are sent to the respective node's kubelet directly by the api server. for 
nodeSelector, scheduler is used

===================================================================

NODEAFFINITY

You can constrain a pod so that it can only run on particular set of nodes

nodeselector is going to be deprecated by kubernetes, nodeaffinity is better 
than nodeselector 

vim nodeaffinity.yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod1
  labels:
    app: pod1
spec:
  containers:
  - name: pod1
    image: nginx
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: size
            operator: In
            values: 
            - low
            - medium

vim nodeaffinity1.yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod2
  labels:
    app: pod2
spec:
  containers:
  - name: pod2
    image: nginx
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
          matchExpression:
          - key: size
            operator: In
            values: 
            - dontKnow

your nodes should have relevant labels

*********************************************************************

Liveness, Readiness and Startup Probes

the kubelet uses liveness probe to know when to restart a container.
the kubelet uses readiness probe to know when a container is ready to start 
accepting traffic. a pod is considered to be ready when all of its containers
are ready
the kubelet uses startup probes to know when a container application has started.
if such a probe is configured, it disables liveness and readiness checks until
it succeeds, making sure those probes don't interfere with the application
startup

vim liveness.yaml
apiVersion: v1
kind: Pod
metadata:
  name: liveness-pod
  labels:
    app: liveness-pod
spec:
  containers:
  - name: liveness-container
    image: busybox
    args:
    - /bin/sh
    - -c
    - touch /tmp/healthy; sleep 30; rm -f /tmp/healthy; sleep 600
    livenessProbe:
      exec:
        command:
        - cat
        - /tmp/healthy
      initialDelaySeconds: 5
      periodSeconds: 5

periodSeconds specifies that the kubelet should perform a liveness probe
every 5 seconds 
the initialDelaySeconds field tells the kubelet that it should wait 5 seconds 
before performing the first probe
to perform the probe, the kubelet executes the command cat /tmp/healthy in 
the target container, if the command succeeds, it returns 0 and the kubelet 
considers the container to be alive and healthy. if the command returns a 
non-zero value, the kubelet kills the container and restarts it.
when the container starts it executes this command
/bin/sh -c "touch /tmp/healthy; sleep 30; rm -f /tmp/healthy; sleep 600"

for the first 30 seconds of the container's life there is a /tmp/healthy file 
so during the first 30 seconds the command runs successfully (liveness probe)

DEFINE A LIVENESS HTTP REQUEST
to perform a probe, the kubelet sends an HTTP GET request to the server that
is running in the container and listening on port 8080. if the handler for 
the server's path returns a success code, the kubelet considers the container
to be alive and healthy

DEFINE A TCP LIVENESS PROBE 
it uses a tcp socket, the kubelet will attempt to open a socket to your 
container on the specified port. if it can establish a connection, the 
container is considered healthy



PROTECT SLOW STARTING CONTAINERS WITH STARTUP PROBES

ports:
- name: liveness-port
  containerPort: 8080
  hostPort: 8080
livenessProbe:
  httpGet:
    path: /healthz
    port: liveness-port
  failureThreshold: 1
  periodSeconds: 10
startupProbe:
  httpGet:
    path: /healthz
    port: liveness-port
  failureThreshold: 30
  periodSeconds: 10


DEFINE READINESS PROBE
a pod with containers reporting that they are not ready does not receive 
traffic through kubernetes service
readiness probe runs on the container during its whole lifecycle
liveness probes do not wait for readiness probe to succeed. if you want to 
wait before executing a liveness probe you should use initialDelaySeconds 
or a startupProbe
readiness and liveness probes can be used together in parallel for the same
container. using both can ensure that the traffic does not reach a container 
that is not ready for it, and that containers are restarted when they fail 


***************************************************************************

REQUESTS AND LIMITS

when you specify a pod, you can optionally specify how much of each resource 
a container needs. the most common resources to specify are CPU and memory (RAM)

requests: kube-scheduler uses this info to decide which node to place the pod on.
limit: the kubelet ensures that the running container is not allowed to use
more of that resource than the limit you set. the kubelet also reserves at least 
the request amount of that system resource specifically for that container to use

cpu and memory are collectively referred to as compute resources

1.0 = one core
100m = 100 milli core

the following pod has 2 containers. both containers are defined with a request
for 0.25 CPU and 64MiB of memory. each container has a limit of 0.5 CPU and
128MiB of memory. you can say that the pod has a request of 0.5 CPU and 128 MiB
of memory and a limit of 1 CPU and 256 MiB of memory

vim pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod1
  labels:
    app: pod1
spec:
  containers:
  - name: c1
    image: nginx
    resources:
      requests:
        cpu: "250m"
        memory: "64Mi"
      limits:
        cpu: "500m"
        memory: "128Mi"
  - name: c2
    image: nginx
    resources:
      requests:
        cpu: "250m"
        memory: "64Mi"
      limits:
        cpu: "500m"
        memory: "128Mi"

**********************************************************************

EXPOSE POD INFORMATION TO CONTAINERS THROUGH ENVIRONMENT VARIABLES

https://kubernetes.io/docs/tasks/inject-data-application/environment-variable-expose-pod-information/

use pod fields as values for environment variables
in the following yaml file you project pod-level fields into the running 
container as environment variables

vim pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod1
spec:
  containers:
  - name: test
    image: registry.k8s.io/busybox
    command: ["sh", "-c"]
    args:
    - while true; do
        echo -en '\n';
        printenv MY_NODE_NAME MY_POD_NAME MY_POD_NAMESPACE;
        printenv MY_POD_IP MY_POD_SERVICE_ACCOUNT;
        sleep 10;
      done;
    env:
      - name: MY_NODE_NAME
        valueFrom: 
          fieldRef:
            fieldPath: spec.nodeName
      - name: MY_POD_NAME
        valueFrom: 
          fieldRef:
            fieldPath: metadata.name
      - name: MY_POD_NAMESPACE
        valueFrom:
          fieldRef:
            fieldPath: metadata.namespace
      - name: MY_POD_IP
        valueFrom: 
          fieldRef:
            fieldPath: status.podIP
      - name: MY_POD_SERVICE_ACCOUNT
        valueFrom:
          fieldRef: 
            fieldPath: spec.serviceAccountName
  restartPolicy: Never

kubectl apply -f pod.yaml
kubectl get pods
kubectl logs podname

kubectl exec -it podname -- sh
printenv


use container fields as values for environment variables
in the following yaml file, you are going to pass fields that are part
of the pod definition, but taken from the specific container rather than
from the pod overall

vim pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod1
spec:
  restartPolicy: Never
  containers:
  - name: pod1
    image: registry.k8s.io/busybox:1.24
    command: ["sh", "-c"]
    args:
    - while true; do
        echo -en '\n';
        printenv MY_CPU_REQUEST MY_CPU_LIMIT;
        printenv MY_MEM_REQUEST MY_MEM_LIMIT;
        sleep 10;
      done;
    resources:
      requests:
        memory: "32Mi"
        cpu: "125m"
      limits:
        memory: "64Mi"
        cpu: "250m"
    env: 
      - name: MY_CPU_REQUEST
        valueFrom: 
          resourceFieldRef: 
            containerName: pod1
            resource: requests.cpu
      - name: MY_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            containerName: pod1
            resource: limits.cpu
      - name: MY_MEM_REQUEST
        valueFrom: 
          resourceFieldRef:
            containerName: pod1
            resource: requests.memory
      - name: MY_MEM_LIMIT
        valueFrom: 
          resourceFieldRef:
            containerName: pod1
            resource: limits.memory

kubectl apply -f pod.yaml
kubectl get pods
kubectl logs pod1