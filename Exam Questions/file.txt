Q) Create a new pod called admin-pod with image busybox. Allow the pod to be 
able to set system_time. The container should sleep for 3200 seconds.

kubectl run admin-pod --image=busybox --command sleep 3200 --dry-run=client -o yaml

-----pod.yaml-----
apiVersion: v1
kind: Pod
metadata:
    name: admin-pod
    labels:
        run: admin-pod
spec:
    containers:
    - name: admin-pod
      image: busybox
      command:
      - sleep
      - "3200"
      securityContext:
        capabilities:
            add: ["SYS_TIME"]

kubectl apply -f pod.yaml

****************************************************************************

Q) A kubeconfig file called test.kubeconfig has been created in /root/TEST. 
There is something wrong with the configuration. Troubleshoot and fix it.

kubectl config view  //get the current config details using this command

aise questions me port, namespace, user, etc aisi values check karna hota hai, 
also compare the given kubeconfig file with the output of kubectl config view

***************************************************************************

Q) Create a new deployment called web-proj-268 with image nginx:1.16 and 1
replica. Next upgrade the deployment to version 1.17 using rolling update.
Make sure that the version upgrade is recorded in the resource annotation.

kubectl create deployment web-proj-268 --image=nginx:1.16

by default the update strategy is set to rolling update and replica is set to 1

update the image using the following command
kubectl set image deployment web-proj-268 nginx=nginx:1.17 --record=true 
the container name in this case is nginx, ( use kubectl describe deployment web-proj-268)
command to check the name of the container. that's why we have used nginx=nginx:1.17

check the image version using the below command
kubectl describe deployment web-proj-268 | grep -i image

check the history of rollout using the following command
kubectl rollout history deployment web-proj-268

************************************************************************

Q) Create a new deployment called web-003. Scale the deployment to 3 replicas.
Make sure the desired number of pods are always running.

kubectl create deployment web-003 --image=nginx --replicas=3

okay, so in exam this command will execute but the pods will not be created,
because the kube controller manager pod is not running. so we need to fix the
kube controller manager pod first and then execute this command. learning: after
running any command make sure to check whether the pod is created or not, even 
after a successful execution, it is not necessary that the pod is created, just
like this question.

controller manager brings the actual number of pods to the desired number
of pods.

check for the kube-controller-manager pod via the following command
kubectl get pods -n kube-system

get logs of controller manager pod using the following command
kubectl logs kube-controller-manager-minikube -n kube-system
if there are no logs then you can describe the pod as well using the below command

kubectl describe pod kube-controller-manager-controlplane -n kube-system

use the below command to find the manifests file of all the system pods
cd /etc/kubernetes/manifests/
ls
cat kube-controller-manager.yaml

toh basically iss type ke question me kisi system pod ke manifest me koi
gadbad hogi jise hame theek karna hai, for that we first need to identify ke
problem kisme hai and kya hai

if a simple question has a high weightage then there is some tricky thing
involved there, so you must review whether the pod is actually created or not


******************************************************************************

Q) Upgrade the cluster (master and worker node) from 1.18.0 to 1.19.0
Make sure to first drain both node and make it available after upgrade

follow the steps below

kubectl drain controlplannode --ignore-daemonsets
apt update (on control plane node)
apt install kubeadm=1.19.0-00
kubeadm upgrade apply v1.19.0
apt install kubelet=1.19.0-00
systemctl restart kubelet
kubectl uncordon controlplanenode
// master upgrade is done

kubectl drain node01 --ignore-daemonsets
ssh node01
apt update
apt install kubeadm=1.19.0-00
kubeadm upgrade node
apt install kubelet=1.19.0-00
systemctl restart kubelet
press ctrl+d to come out of node01
kubectl uncordon node01
*************************************************************************

Q) Deploy a web-load-5461 pod using the nginx:1.17 image with the labels set 
to tier=web

** pod.yaml **
apiVersion: v1
kind: Pod
metadata:
    name: podfile
    labels:
        tier: web
spec:
    containers:
    - name: nginx
      image: nginx:1.17

kubectl apply -f pod.yaml

or you can use the below command

kubectl run web-load-5461 --image=nginx:1.17 --labels tier=web
kubectl get pods --show-labels

***************************************************************

Q) create a static pod on node01 called static-nginx with image nginx and 
you have to make sure that it is recreated/restarted automatically in case of
any failure happens 

types of questions on static pod
1) You will be given with all the information and will be asked to schedule a
static pod on any node (worker or master)
2) You will be asked to delete a running static pod (unlike normal pods you can only
 delete a running static pod by deleting its manifest file)
3) You will be asked to move a static pod running on master node to worker node
(move the file)

how to identify a static pod: static pod has a suffix of node name in pod name

how to identify the location where kubelet is running
ssh node01
ps -aux | grep kubelet
find the kubelet config.yaml file, it is present at /var/lib/kubelet/config.yaml
in this file you will see a key 'staticPodPath', this is the location where
staticPod should be created (/etc/kubernetes/manifests). so at this location
you should create the yaml file for static pod

if you see this location in master node, you will see the manifests file for
etcd, apiserver, controller manager and scheduler. kubeadm deploys the control
plane components as a static pod

static pods can only be deleted if we delete its yaml file

kubelet will only be able to create static pod, if you try to write a deployment
file in /etc/kubernetes/manifests, it will not work
*************************************************************************

q) create a pod called multi-pod with two containers with below description
    c1: name container1, image nginx
    c2: name container2, image busybox, command sleep 4800

apiVersion: v1
kind: Pod
metadata:
  name: multi-pod
  labels:
    env: dev
spec:
  containers:
  - name: container1
    image: nginx
  - name: container2
    image: busybox
    command: ["sleep", "4800"]

**************************************************************************

q) create a pod called delta-pod in defense namespace belonging to the 
development environment (env=dev) and frontend tier (tier=front), image=nginx:1.17

apiVersion: v1
kind: Pod
metadata:
  name: delta-pod
  label:
    env: dev
    tier: front
  namespace: defense 
spec:
  containers:
  - image: nginx:1.17
     name: delta-pod

after completing a question, always verify whether the appropriate changes
have been made or not, in this case, check whether the pod has been created 
in the correct namespace or not with the correct image. use the following command
to check

kubect describe pod delta-pod -n defense | grep -i image

*********************************************************

Q) get the node node01 in JSON format and store it in a file at /opt/outputs/nodes-fz456723je.json

kubectl get nodes node01 -o json > /opt/outputs/nodes-fz456723je.json
 
(make sure there is a directory /opt/outputs, if not then create one)

*********************************************************

q) take a backup of the etcd database and save it to root with name "etcd-backup.db"

search etcd snapshot in documentation

ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 \
  --cacert=<trusted-ca-file> --cert=<cert-file> --key=<key-file> \
  snapshot save <backup-file-location>

ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key \
  snapshot save etcd-backup.db

etcd-backup.db is the file location that is given in question

****************************************************************************

Q) a new application finance-audit-pod is deployed in finance namespace.
There is something wrong with it. identify and fix the issue.

note: no configuration changes allowed, you can only delete and recreate the pod
(if required)

kubectl -n finance describe pod finance-audit-pod

yeh karke pod ko detail me dekho pehle toh

under events you will see some logs and usme error dikhega

isme sleep ka type tha sheep kar diya tha. so as per question, you cannot
make any config changes, you need to delete this pod and recreate a pod bu
creating a new yaml file for the pod

to delete a pod use the below command, it works faster this way

kubectl -n finance delete pod finance-audit-pod --grace-period=0 --force

make sure to run kubectl get pods -n finance to check whether the pod is 
actually removed or not

***********************************************************

q) create a pod called web-pod using image nginx, expose it internally with
a service called web-pod-svc. check that you are able to look up the service
and pod from within the cluster
use the image: busybox:1.28 for dns lookup
record results in /root/web-svc.svc and /root/web-pod.pod

vim 1-pod.yaml

apiVersion: v1
kind: Pod
metadata:
  name: web-pod
  app: web-pod
spec:
  containers:
  - name: web-pod
    image: nginx
    ports:
    - containerPort: 80

vim 1-service.yaml

apiVersion: v1
kind: Service
metadata:
  name: serviceyamlfile
spec:
  selector:
    app: web-pod
  ports:
  - protocol: TCP
    targetPort: 80
    port: 80

kubectl apply -f 1-pod.yaml
kubectl apply -f 1-service.yaml

kubectl run lookuppod --image=busybox:1.28

kubectl get pods -o wide //note the ipa of the pod

kubectl exec -it lookuppod -- nslookup web-pod-svc > /root/web-svc.svc
kubectl exec -it lookuppod -- nslookup 192-168-10-8.default.pod > /root/web-pod.pod

for pods you need to provide its ipa, octets separated by - and followed by
namespace (default in this case) and then followed by pod

*************************************************************************

q) use JSON PATH query to retrieve the osImages of all the nodes and store
it in a file "allNodes_osImage_45CVB34ji.txt" at root location

note: the osImages are under the nodeinfo section under status of each node

kubectl get nodes -o=jsonpath='{.items[*].status.nodeInfo.osImage}' > file.txt


****************************************************************************

q) create a persistent volume with the given specification
volume name: pv-rnd
storage: 100Mi
access modes: ReadWriteMany
host path: /pv/host_data-rnd

vim pv.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-rnd
  labels:
    ques: three
spec:
  storageClassName: manual
  capacity: 
    storage: 100Mi
  accessModes: 
    - ReadWriteMany
  hostPath:
    path: "/pv/host_data-rnd" 

******************************************************************

q) expose the "audit-web-app" web pod as service "audit-web-app-service" 
application on port 30002 on the nodes on the cluster

note: the web application listens on port 8080

vim 1-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: audit-web-app
  labels:
    app: audit-web-app
spec:
  containers:
  - name: audit-web-app
    image: nginx
    ports:
    - containerPort: 8080

vim 1-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: audit-web-app-service
spec:
  selector:
    app: audit-web-app
  type: NodePort
  ports:
    - port: 8080
      targetPort: 8080
      nodePort: 30002

always check your answers. check whether the pod and service are running or not
and describe the service to see its endpoint and verify that it is that pod only

******************************************************************************

q) Taint the worker node node01 with details provided below. create a pod called
dev-pod-nginx using image=nginx, make sure that workloads are not scheduled 
to this worker node node01. create another pod called prod-pod-nginx using
image=nginx with toleration to be scheduled on node01.
details: key:env_type, value:production, operator:Equal and effect:NoSchedule

kubectl taint node node01 env_type=production:NoSchedule

vim 1-pod1.yaml
apiVersion: v1
kind: Pod
metadata:
  name: dev-pod-nginx
  labels:
    app: dev-pod-nginx
spec:
  containers:
  - name: dev-pod-nginx
    image: nginx

vim 1-pod2.yaml
apiVersion: v1
kind: Pod
metadata:
  name: prod-pod-nginx
  labels:
    app: prod-pod-nginx
spec:
  containers:
  - name: prod-pod-nginx
    image: nginx
  tolerations:
  - key: "env_type"
    operator: "Equal"
    value: "production"
    effect: "NoSchedule"

kubectl apply -f 1-pod1.yaml
kubectl apply -f 1-pod2.yaml

kubectl get pods -o wide

*******************************************************************

q) create a pod called pod-jxc56fv using the details below
securityContext 
runAsUser: 1000
fsGroup: 2000

image: redis:alpine

vim pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-jxc56fv
  labels:
    app: pod-jxc56fv
spec:
  containers:
  - name: pod-jxc56fv
    image: redis:alpine
  securityContext:
    runAsUser: 1000
    fsGroup: 2000

kubectl apply -f pod.yaml

****************************************************************************

q) worker node node01 is not responding, have a look and fix the issue

you need to think ke konse processed worker node pe chalte hain jo agar kharab
ho jaye toh worker node ruk jayega, its kubelet and kubeproxy, their pods and
services

kubectl get nodes
-> not ready 

kubectl describe node node01

try to find any error here

if not then ssh to the node

ssh node01

check the status of kubelet using the below command
systemctl status kubelet

you will find some error logs there

systemctl start kubelet
use the above command to start kubelet

itna karne yeh question toh solve ho gaya

************************************************************************

Q) list the InternalIP of all nodes of the cluster. save the result to a file
/root/Internal_IP_List

answer should be in the format: InternalIP of First Node<space>InternalIP of 
Second Node (in a single line)

aise questions me jsonpath use hota hai and its a good idea to search 
cheat sheet in k8s docs

example externalip ka hai doc me so you have to ctrl+f for externalip, itna
dimaag chalna chahiye ki external ka bhi search kare along with internal cauz
json output me dono ka placement ek jaisa hi hona chahiye

kubectl get nodes -o jsonpath='{.items[*].status.addresses[?(@.type=="InternalIP")].address}' > /root/Internal_IP_List

**********************************************************************

Q) one static pod "web-static", image busybox, is currently running on controlplane
node, move that static pod to run on node01, don't need to do any other changes

note: static pod name should be changed from web-static-controlplane to 
web-static-node01

--
note is just to confuse you, node name is already appened to the static pod
name

ps -aux | grep kubelet
cat /var/lib/kubelet/config.yaml
cd /etc/kubernetes/manifests
vim pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: web-static
  labels:
    app: web-static
spec:
  containers:
  - name: web-static
    image: busybox
    command: ["sleep", "10000"]

kubectl get pods

ssh node01
cd /etc/kubernetes/manifests
vim pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: web-static
  labels:
    app: web-static
spec:
  containers:
  - name: web-static
    image: busybox
    command: ["sleep", "10000"]

*************video8*********************************************************

Q) A new user named "alok" need to be created. grant him access to the cluster.
user "alok" should have permission to create, list, get, update and delete pods
in the space namespace. the private key exists at location: /root/alok.key and
csr at /root/alok.csr

search 'certificate signing request' in k8s docs

copy the csr yaml file from the doc
vim csr.yaml
apiVersion: certificates.k8s.io/v1
kind: CertificateSigningRequest
metadata:
  name: alok
spec:
  request:
  signerName: kubernetes.io/kube-apiserver-client
  usages:
  - client auth

request is base64 encoded value of the csr content 
cat alok.csr | base64 | tr -d "\n"

kubectl apply -f csr.yaml

kubectl get csr

to check whether the user alok is allowed to get pods from the namespace space
run the following command
kubectl get pods -n space --as alok
this will through an error

the csr is still in pending state so you need to approve it

kubectl certificate approve alok 

now create role and rolebinding

vim role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: space
  name: pod-reader
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["create","get","update","list","delete"]

kubectl apply -f role.yaml

vim rb.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadat:
  name: read-pods
  namespace: space
subjects:
- kind: User
  name: alok
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io

kubectl apply -f rb.yaml

kubectl get pods -n space --as alok
you won't find any error this time

you can also use the below command to test the permission of alok
kubectl auth can-i get pods -n space --as alok
yes

*********************************************************************

Q) create a persistent volume, presistentvolumeclaim, pod with below specifications
pv
volume name: mypvlog
storage: 100Mi
access modes: ReadWriteMany
host path: /pv/log
reclaim policy: Retain

pvc
volume name: pv-claim-log
storage request: 50Mi
access modes: ReadWriteMany

pod
name: my-nginx-pod
image: nginx
volume: PersistentVolumeClaim=pv-claim-log
volume mount: /log

vim pv.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: mypvlog
  labels:
    type: local
spec:
  storageClassName: manual
  capacity:
    storage: 100Mi
  accessModes: 
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  hostPath:
    path: /pv/log

vim pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pv-claim-log
spec:
  storageClassName: manual
  accessModes:
    - ReadWriteMany
  resources:
    requests: 
      storage: 50Mi

vim pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-nginx-pod
spec:
  containers:
  - name: my-nginx-pod
    image: nginx
    volumeMounts: 
      - mountPath: "/log"
        name: v1
  volumes:
    - name: v1
      persistentVolumeClaim: 
        claimName: pv-claim-log

kubectl apply -f pv.yaml
kubectl apply -f pvc.yaml
kubectl apply -f pod.yaml

***********************************************************************

Q) worker node node01 is not responding, have a look and fix the issue

kubectl get nodes
kubectl describe nodes node01
ssh node01
systemctl status kubelet
systemctl restart kubelet

agar iske baad start ho jata hai toh theek otherwise

journalctl -u kubelet
look for any failure related msg
press ctrl+g to load more msg, press enter

cat /var/lib/kubelet/config.yaml
update the ca cert file location
systemctl daemon-reload
systemctl restart kubelet

systemctl status kubelet
kubectl get nodes
************************************************************************

Q) a pod "my-nginx-pod" (image=nginx) in custom namespace is not running. Find
the problem and fix it and make it running. all the supported definitions files 
has been placed at root

kubectl get pods -n custom
kubectl describe pod my-nginx-pod -n custom

under the events you can see the reason, in this case pvc not found, so check
for pv and pvc

kubectl get pv
kubectl get pvc -n custom

pvc also needs to be in the same namespace as of pod, ie custom

kubectl get pvc 

delete the pvc and edit the pvc yaml file to include the namespace: custom

you need to delete pv as well, cauz uska status released hoga pvc delete karne
ke baad and we need its status to be available

must verify the final status of the pod

***************************************************************************

Q) create a multi-container pod, "multi-pod" in development namespace using
images nginx and redis

kubectl create ns development

vim pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: multi-pod
  labels:
    app: multi-pod
  namespace: development
spec:
  containers:
  - name: nginx
    image: nginx
  - name: redis
    image: redis

kubectl apply -f pod.yaml
kubectl get pods -n development
kubectl describe pod multi-pod -n development

**********************************************************************

Q) a pod "nginx-pod" (image=nginx) in default namespace is not running. find
the problem and fix it and make it running.

kubectl get pods
kubectl describe pod nginx-pod

under events you can see that both the nodes had taint that the pod didn't tolerate
so either you can add toleration to the pod or remove taint. addind toleration
is a good idea

kubectl get nodes -o wide
kubectl describe node node01 | grep -i taint
kubectl describe node controlplane | grep -i taint

find the taint information for the node, effect, key:value


vim pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod
  labels:
    app: nginx-pod
spec:
  containers:
  - name: nginx-container
    image: nginx
  tolerations:
  - key: "color"
    operator: "Equal"
    value: "blue"
    effect: "NoSchedule"
  
if you don't find the yaml file for any entity then just create it
kubectl get pod nginx-pod -o yaml > pod.yaml
aise cases me kabhi bhi naya pod.yaml nahi banana cauz running container
me kaafi saare infos ho sakte hain jo tum otherwise miss kar jao
so only use the above command to get the output in the yaml format. 
ya phir ek baar dekh lena ki kya kya info hai usme, agar koi imp lage toh 
zaroor use karna

********************************video 10*************************************

q) 